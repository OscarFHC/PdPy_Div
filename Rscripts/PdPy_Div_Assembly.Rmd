---
title: "EAS 538 Chi-square & ANOVA"
author: "Oscar Feng-Chang"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    code_folding: show
    highlight: textmate
    keep_md: no
    number_sections: yes
    theme: flatly
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
      toc_depth: 4
  pdf_document:
    toc: yes
---

```{r, set global theme, include=FALSE}
knitr::opts_chunk$set(fig.width=10, fig.height=10)
```

```{r, library loading, echo=FALSE, message=FALSE}
if (!require(rmarkdown)) {
  install.packages("rmarkdown", dependencies=TRUE, repos = 'http://cran.us.r-project.org')
  library(rmarkdown)
}else{library(rmarkdown)}

if (!require(knitr)) {
  install.packages("knitr", dependencies=TRUE, repos = 'http://cran.us.r-project.org')
  library(knitr)
}else{library(knitr)}

if (!require(tidyverse)) {
  install.packages("tidyverse", dependencies=TRUE, repos = 'http://cran.us.r-project.org')
  library(tidyverse)
}else{library(tidyverse)}

if (!require(vegan)) {
  install.packages("vegan", dependencies=TRUE, repos = 'http://cran.us.r-project.org')
  library(vegan)
}else{library(vegan)}

if (!require(ape)) {
  install.packages("ape", dependencies=TRUE, repos = 'http://cran.us.r-project.org')
  library(ape)
}else{library(ape)}

if (!require(parallel)) {
  install.packages("vegan", dependencies=TRUE, repos = 'http://cran.us.r-project.org')
  library(parallel)
}else{library(parallel)}

if (!require(SpadeR)) {
  install.packages("SpadeR", dependencies=TRUE, repos = 'http://cran.us.r-project.org')
  library(SpadeR)
}else{library(SpadeR)}

if (!require(iNEXT)) {
  install.packages("iNEXT", dependencies=TRUE, repos = 'http://cran.us.r-project.org')
  library(iNEXT)
}else{library(iNEXT)}

if (!require(picante)) {
  install.packages("picante", dependencies=TRUE, repos = 'http://cran.us.r-project.org')
  library(picante)
}else{library(picante)}

if (!require(geiger)) {
  install.packages("geiger", dependencies=TRUE, repos = 'http://cran.us.r-project.org')
  library(geiger)
}else{library(geiger)}

if (!require(GUniFrac)) {
  install.packages("GUniFrac", dependencies=TRUE, repos = 'http://cran.us.r-project.org')
  library(GUniFrac)
}else{library(GUniFrac)}

if (!require(phytools)) {
  install.packages("phytools", dependencies=TRUE, repos = 'http://cran.us.r-project.org')
  library(phytools)
}else{library(phytools)}

if (!require(ecodist)) {
  install.packages("ecodist", dependencies=TRUE, repos = 'http://cran.us.r-project.org')
  library(ecodist)
}else{library(ecodist)}

if (!require(ade4)) {
  install.packages("ade4", dependencies=TRUE, repos = 'http://cran.us.r-project.org')
  library(ade4)
}else{library(ade4)}

# install.packages("iNEXT")
# 
# ## install the latest version from github
# install.packages('devtools')
# library(devtools)
# install_github('JohnsonHsieh/iNEXT')
# library(iNEXT)

if (!require(lavaan)) {
  install.packages("lavaan", dependencies=TRUE, repos = 'http://cran.us.r-project.org')
  library(lavaan)
}else{library(lavaan)}

if (!require(nlme)) {
  install.packages("nlme", dependencies=TRUE, repos = 'http://cran.us.r-project.org')
  library(nlme)
}else{library(nlme)}

if (!require(piecewiseSEM)) {
  install.packages("piecewiseSEM", dependencies=TRUE, repos = 'http://cran.us.r-project.org')
  library(piecewiseSEM)
}else{library(piecewiseSEM)}

if (!require(brms)) {
  install.packages("brms", dependencies=TRUE, repos = 'http://cran.us.r-project.org')
  library(brms)
}else{library(brms)}

if (!require(blavaan)) {
  install.packages("blavaan", dependencies=TRUE, repos = 'http://cran.us.r-project.org')
  library(blavaan)
}else{library(blavaan)}

if (!require(rstanarm)) {
  install.packages("rstanarm", dependencies=TRUE, repos = 'http://cran.us.r-project.org')
  library(rstanarm)
}else{library(rstanarm)}

library("rstan")
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)

if (!require(loo)) {
  install.packages("loo", dependencies=TRUE, repos = 'http://cran.us.r-project.org')
  library(loo)
}else{library(loo)}

if (!require(abind)) {
  install.packages("abind", dependencies=TRUE, repos = 'http://cran.us.r-project.org')
  library(abind)
}else{library(abind)}

if (!require(ggplot2)) {
  install.packages("ggplot2", dependencies=TRUE, repos = 'http://cran.us.r-project.org')
  library(ggplot2)
}else{library(ggplot2)}

if (!require(viridis)) {
  install.packages("viridis", dependencies=TRUE, repos = 'http://cran.us.r-project.org')
  library(viridis)
}else{library(viridis)}

if (!require(GGally)) {
  install.packages("GGally", dependencies=TRUE, repos = 'http://cran.us.r-project.org')
  library(GGally)
}else{library(GGally)}

if (!require(cowplot)) {
  install.packages("cowplot", dependencies=TRUE, repos = 'http://cran.us.r-project.org')
  library(cowplot)
}else{library(cowplot)}
```

```{r, read in data}
Bac_comm <- as.data.frame(t(read.table(file = "D:/Research/PdPy_Div/data/16s_seqtab.csv", sep = ",", 
                                       header = TRUE, row.names = 1, stringsAsFactors = FALSE, fill = TRUE)))
Bac_ra_comm <- Bac_comm / rowSums(Bac_comm)
Bac_phylo<- read.tree(file = "D:/Research/PdPy_Div/data/treeNJ_16s.tree")

NF_comm <- as.data.frame(t(read.table(file = "D:/Research/PdPy_Div/data/18s_seqtab.csv", sep = ",", 
                                      header = TRUE, row.names = 1, stringsAsFactors = FALSE, fill = TRUE)))
NF_ra_comm <- NF_comm / rowSums(NF_comm)
NF_phylo<- read.tree(file = "D:/Research/PdPy_Div/data/treeNJ_18s.tree")

Vars <- read.table(file = "D:/Research/PdPy_Div/data/sECS_Vars.csv", sep = ",", 
                         header = TRUE, stringsAsFactors = FALSE, fill = TRUE)
```


```{r}
Bac_A <- iNEXT(t(Bac_comm), q = 0, datatype = "abundance", size = max(colSums(Bac_comm)) + 100000)$AsyEst

Bac_Div <-  Bac_A %>% select(Site, Diversity, Estimator) %>% spread(Diversity, Estimator) %>%
  rename(SR = "Species richness", Shannon = "Shannon diversity", Simpson = "Simpson diversity") 
```


# Phylogenetic turnover / deterministic vs stochastic processes

```{r, Bacteria phylogenetic turnover}
numCores <- detectCores()
numCores

cl <- makeCluster(numCores)

clusterEvalQ(cl, {
  library(vegan)
  library(tidyverse)
  library(picante)
  Bac_comm <- as.data.frame(t(read.table(file = "D:/Research/PdPy_Div/data/16s_seqtab.csv", sep = ",", 
                                       header = TRUE, row.names = 1, stringsAsFactors = FALSE, fill = TRUE)))
  Bac_phylo<- read.tree(file = "D:/Research/PdPy_Div/data/treeNJ_16s.tree")

})

BacPhylo_null_func <- function(x){
  #set up parameters and data
  community = Bac_comm
  phylo = Bac_phylo
  # performing comdistnt to calculate MNTD
  as.matrix(comdistnt(community, cophenetic(tipShuffle(phylo)), abundance.weighted = TRUE))
}

nsim.list <- sapply(1:999, list)
test <- parLapply(cl, nsim.list, BacPhylo_null_func)
test[[1000]] <- as.matrix(comdistnt(Bac_comm, cophenetic(Bac_phylo), abundance.weighted = TRUE))

BacPhylo_null <- data.frame(matrix(unlist(test), ncol = length(test), byrow = FALSE)) %>%
  cbind(expand.grid(row.names(Bac_comm), row.names(Bac_comm))) %>%
  rename(obs = X1000)
write.table(BacOTU_null, file = "D:/Research/PdPy_Div/Results/Bac_MNTD_null.csv", 
            sep = ",", col.names = TRUE, row.names = FALSE)
stopCluster(cl)
```

```{r, Nanoflagellate phylogenetic turnover}
numCores <- detectCores()
numCores

cl <- makeCluster(numCores)

clusterEvalQ(cl, {
  library(vegan)
  library(tidyverse)
  library(picante)
  NF_comm <- as.data.frame(t(read.table(file = "D:/Research/PdPy_Div/data/18s_seqtab.csv", sep = ",", 
                                      header = TRUE, row.names = 1, stringsAsFactors = FALSE, fill = TRUE)))
  NF_phylo<- read.tree(file = "D:/Research/PdPy_Div/data/treeNJ_18s.tree")
})

NFPhylo_null_func <- function(x){
  # set up parameters and data
  community = NF_comm
  phylo = NF_phylo
  # performing comdistnt to calculate MNTD
  as.matrix(comdistnt(community, cophenetic(tipShuffle(phylo)), abundance.weighted = TRUE))
}

nsim.list <- sapply(1:999, list)
test <- parLapply(cl, nsim.list, NFPhylo_null_func)
test[[1000]] <- as.matrix(comdistnt(NF_comm, cophenetic(NF_phylo), abundance.weighted = TRUE))

NFPhylo_null <- data.frame(matrix(unlist(test), ncol = length(test), byrow = FALSE)) %>%
  cbind(expand.grid(row.names(NF_comm), row.names(NF_comm))) %>%
  rename(obs = X1000)
# write.table(NFOTU_null, file = "D:/Research/PdPy_Div/Results/NF_MNTD_null.csv", 
#             sep = ",", col.names = TRUE, row.names = FALSE)
stopCluster(cl)
```

# OTU turnover / Dispersal limitation vs homogeneous dispersal

```{r, Bacteria OTU turnover}
numCores <- detectCores()
numCores

cl <- makeCluster(numCores)

clusterEvalQ(cl, {
  library(vegan)
  library(tidyverse)
  library(picante)
  Bac_comm <- as.data.frame(t(read.table(file = "D:/Research/PdPy_Div/data/16s_seqtab.csv", sep = ",", 
                                       header = TRUE, row.names = 1, stringsAsFactors = FALSE, fill = TRUE)))
})

BacOTU_null_func <- function(x){
  # set up parameters and data
  community = Bac_comm
  n.mod = "independentswap"
  method = "chao"
  # performing comdistnt to calculate MNTD
  as.matrix(vegdist(randomizeMatrix(community, null.model = n.mod), method = method))
}

nsim.list <- sapply(1:999, list)
test <- parLapply(cl, nsim.list, BacOTU_null_func)
test[[1000]] <- as.matrix(vegdist(Bac_comm, null.model = "independentswap", method = "chao"))

BacOTU_null <- data.frame(matrix(unlist(test), ncol = length(test), byrow = FALSE)) %>%
  cbind(expand.grid(row.names(Bac_comm), row.names(Bac_comm))) %>%
  rename(obs = X1000)
# write.table(BacOTU_null, file = "D:/Research/PdPy_Div/Results/Bac_chao_null.csv", 
#             sep = ",", col.names = TRUE, row.names = FALSE)
stopCluster(cl)
```

```{r, Nanoflagellate OTU turnover}
numCores <- detectCores()
numCores

cl <- makeCluster(numCores)

clusterEvalQ(cl, {
  library(vegan)
  library(tidyverse)
  library(picante)
  NF_comm <- as.data.frame(t(read.table(file = "D:/Research/PdPy_Div/data/18s_seqtab.csv", sep = ",", 
                                        header = TRUE, row.names = 1, stringsAsFactors = FALSE, fill = TRUE)))
})

NFOTU_null_func <- function(x){
  # set up parameters and data
  community = NF_comm
  n.mod = "independentswap"
  method = "chao"
  # performing comdistnt to calculate MNTD
  as.matrix(vegdist(randomizeMatrix(community, null.model = n.mod), method = method))
}

nsim.list <- sapply(1:999, list)
test <- parLapply(cl, nsim.list, NFOTU_null_func)
test[[1000]] <- as.matrix(vegdist(NF_comm, null.model = "independentswap", method = "chao"))

NFOTU_null <- data.frame(matrix(unlist(test), ncol = length(test), byrow = FALSE)) %>%
  cbind(expand.grid(row.names(NF_comm), row.names(NF_comm))) %>%
  rename(obs = X1000)
write.table(NFOTU_null, file = "D:/Research/PdPy_Div/Results/NF_chao_null.csv", 
            sep = ",", col.names = TRUE, row.names = FALSE)
stopCluster(cl)
```


# Phylogenetic signal

```{r, Bacteria phylogenetic signal, eval = FALSE}
Bac_Niche <- as.data.frame(matrix(0, ncol(Bac_comm), 8)) %>%
  #mutate(Bac_ID = names(Bac_comm)) %>%
  rename(Temp = V1, Sal = V2, PAR = V3, NO2 = V4, NO3 = V5, DIN = V6, PO3 = V7, Chla = V8) 

for(i in 1:ncol(Bac_comm)){ 
  Bac_Niche[i, "Temp"] = sum(Bac_comm[,names(Bac_comm)[i]] * Vars$Temp) / sum(Bac_comm[,names(Bac_comm)[i]])
  Bac_Niche[i, "Sal"] = sum(Bac_comm[,names(Bac_comm)[i]] * Vars$Sal) / sum(Bac_comm[,names(Bac_comm)[i]])
  Bac_Niche[i, "PAR"] = sum(Bac_comm[,names(Bac_comm)[i]] * Vars$PAR) / sum(Bac_comm[,names(Bac_comm)[i]])
  Bac_Niche[i, "NO2"] = sum(Bac_comm[,names(Bac_comm)[i]] * Vars$NO2) / sum(Bac_comm[,names(Bac_comm)[i]])
  Bac_Niche[i, "NO3"] = sum(Bac_comm[,names(Bac_comm)[i]] * Vars$NO3) / sum(Bac_comm[,names(Bac_comm)[i]])
  Bac_Niche[i, "DIN"] = sum(Bac_comm[,names(Bac_comm)[i]] * Vars$DIN) / sum(Bac_comm[,names(Bac_comm)[i]])
  Bac_Niche[i, "PO3"] = sum(Bac_comm[,names(Bac_comm)[i]] * Vars$PO3) / sum(Bac_comm[,names(Bac_comm)[i]])
  Bac_Niche[i, "Chla"] = sum(Bac_comm[,names(Bac_comm)[i]] * Vars$Chla) / sum(Bac_comm[,names(Bac_comm)[i]])
}

Bac_Niche <- as.data.frame(scale(Bac_Niche, center = TRUE, scale = TRUE))
Bac_Niche_Dist <- vegdist(Bac_Niche, method = "euclidean")
Bac_Phylo_Dist <- cophenetic(Bac_phylo)
Bac_PhySig <- mantel.correlog(Bac_Niche_Dist, Bac_Phylo_Dist, n.class = 50)
Bac_PhySig_fig <- plot(Bac_PhySig)
ggsave(plot(Bac_PhySig), file = "D:/Research/PdPy_Div/Results/Bac_PhySig_fig.pdf", dpi = 600) 
```

```{r, Nanoflagellate phylogenetic signal, eval = FALSE}
NF_Niche <- as.data.frame(matrix(0, ncol(NF_comm), 8)) %>%
  #mutate(Bac_ID = names(Bac_comm)) %>%
  rename(Temp = V1, Sal = V2, PAR = V3, NO2 = V4, NO3 = V5, DIN = V6, PO3 = V7, Chla = V8) 

for(i in 1:ncol(NF_comm)){ 
  NF_Niche[i, "Temp"] = sum(NF_comm[,names(NF_comm)[i]] * Vars$Temp) / sum(NF_comm[,names(NF_comm)[i]])
  NF_Niche[i, "Sal"] = sum(NF_comm[,names(NF_comm)[i]] * Vars$Sal) / sum(NF_comm[,names(NF_comm)[i]])
  NF_Niche[i, "PAR"] = sum(NF_comm[,names(NF_comm)[i]] * Vars$PAR) / sum(NF_comm[,names(NF_comm)[i]])
  NF_Niche[i, "NO2"] = sum(NF_comm[,names(NF_comm)[i]] * Vars$NO2) / sum(NF_comm[,names(NF_comm)[i]])
  NF_Niche[i, "NO3"] = sum(NF_comm[,names(NF_comm)[i]] * Vars$NO3) / sum(NF_comm[,names(NF_comm)[i]])
  NF_Niche[i, "DIN"] = sum(NF_comm[,names(NF_comm)[i]] * Vars$DIN) / sum(NF_comm[,names(NF_comm)[i]])
  NF_Niche[i, "PO3"] = sum(NF_comm[,names(NF_comm)[i]] * Vars$PO3) / sum(NF_comm[,names(NF_comm)[i]])
  NF_Niche[i, "Chla"] = sum(NF_comm[,names(NF_comm)[i]] * Vars$Chla) / sum(NF_comm[,names(NF_comm)[i]])
}

NF_Niche <- as.data.frame(scale(NF_Niche, center = TRUE, scale = TRUE))
NF_Niche_Dist <- vegdist(NF_Niche, method = "euclidean")
NF_Phylo_Dist <- cophenetic(NF_phylo)
NF_PhySig <- mantel.correlog(NF_Niche_Dist, NF_Phylo_Dist, n.class = 50)
plot(NF_PhySig)
ggsave(plot(NF_PhySig), file = "D:/Research/PdPy_Div/Results/NF_PhySig_fig.pdf", dpi = 600) 
```









This week in lecture we learned about chi-square tests and ANOVAs. Let's revisit both of these tests in today's lab. 
# Chi-square test of independence
This is a statistical test that you use when your independent and dependent variables are both categorical. Let's say you want to see whether undergraduate students at the University of Michigan have different views on climate change depending on which state they grew up in, Kentucky or New York. You survey 100 students each from Kentucky and New York and create a contingency table that summarizes their views on climate change. Reminder - a contingency table is one where the distribution of one variable is listed in the rows and the distribution of a second variable is listed in the columns. Let's see an example to concretely understand what I mean.

```{r, cs cc,eval=FALSE}
ccyes <- c(75, 62)
ccno <- c(25, 38)
state <- c('New York', 'Kentucky')
contable <- rbind(ccyes, ccno)
colnames(contable) <- state
contable
```
Here `ccyes` are the number of students who said they believe in climate change and `ccno` are the number of students who do not believe in climate change. Before you get upset at my depiction of New Yorkers and Kentuckians, check out this map [http://climatecommunication.yale.edu/visualizations-data/ycom-us-2016/?est=happening&type=diff&geo=state], which is where I got some rough numbers for this fake dataset. 

Is the distribution of climate change beliefs different between New York and Kentucky students? Let's use a chi-square test to find out!
```{r, chisq run,eval=FALSE}
chisq.test(contable)
```

---

__Exercise 1__  
1) Write out the null and alternate hypothesis for the chi-square test you just ran. Please interpret the result. Can you reject the null hypothesis at alpha = 0.05? What about at alpha = 0.10?    
2) Now run the same analyses as above, but pick two different states. Use the same link I used [http://climatecommunication.yale.edu/visualizations-data/ycom-us-2016/?est=happening&type=diff&geo=state] to get information on the % of people who believe global warming is happening in each state in the US. *Hint - look at the numbers for Kentucky and New York. How did I use these to parameterize my `ccyes` and `ccno` values above? Please do the same thing, but pick two different states.* 

---

# One-way ANOVA
Now we're going to learn how to implement one way and two way ANOVAs in R. To do this, let's load the [OrchardSprays data](https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/OrchardSprays.html) which contains information on how different types of orchard sprays repel honeybees. This is a preloaded dataset in R. To learn more about the dataset you can use `?OrchardSprays`.

```{r, read in OrchardSprays data set, collapse=TRUE,eval=FALSE}
head(OrchardSprays)
?OrchardSprays
```
The dataset has 4 columns that represent the following variables:   
`decrease` = the amount of sucrose solution that was eaten by the bees within 2 hours  
`rowpos` = the experiment was run in a grid, and this states what row the treatment was run in  
`colpos` = this states what column the treatment was run in   
`treatment` = this states what treatment was used. 'A' represents the treatment with the highest level of lime sulphur and 'H' represents the treatment with no lime sulphur. 

Let's first visualize the mean effect of each treatment. *What type of plot would you create to visualize differences across treatments?* 

Hopefully you said boxplot! 
```{r, visualization,eval=FALSE}
boxplot(decrease ~ treatment, data = OrchardSprays, xlab = "treatment", ylab = "decrease mean (effect size)")
```

Now let's run an ANOVA to see if treatment has a significant effect on the amount of sucrose the bees eat. 

```{r, simple ANOVA,eval=FALSE}
spray.aov <- aov(decrease ~ treatment, data = OrchardSprays)
summary(spray.aov)
```
Okay, let's interpret the output of this ANOVA. *What does the p value tell us about the effect of treatment on the amount of sugar the bees eat?*

But do we know which specific treatments are different? No! To figure this out, we'll have to perform a post-hoc test to see if certain treatments are significantly different from one another. We will use the [Tukey's honest significant differences](https://en.wikipedia.org/wiki/Tukey%27s_range_test) test, which is a t-test based statistical method that *simultaneously* calculates the confidence intervals of all pair-wise differences across all treatments. The function for Tukey's honest significant differences test in R is `TukeyHSD()`. 

```{r, post-hoc comparison for one way ANOVA,eval=FALSE}
TukeyHSD(spray.aov)
```

---

__Exercise 2__  
1) Which treatments are significantly different from one another according to the results of the TukeyHSD?  
2) Why do we use the Tukey's HSD test instead of just running multiple t-tests that compare each pair of treatments in your sample?   

---

# Two-way ANOVA
Now let's run a two way ANOVA. To do this, let's use a third dataset on the carbon dioxide uptake in grass plants [CO2](https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/zCO2.html). This is another preloaded dataset in R. Let's look at what's in the dataset, and check out more details about the dataset by using `?`.

```{r, read in CO2 data, collapse=TRUE,eval=FALSE}
head(CO2)
?CO2
```
I'm not going to write out what each variable in the dataset is, because there is a detailed description in the R description file in the help menu (what you called with `?`).

First, let's visualize the data to understand how `uptake` rate varies based on `Type` and `Treatment`.

```{r, plot c02,eval=FALSE}
par(mfrow = c(2, 1))
boxplot(uptake ~ Type,data = CO2, las = 1, main = 'Origin of the Plant')
boxplot(uptake ~ Treatment, data = CO2, las = 1, main = 'Treatment')
```
*If you're unsure of what all of the parameters in the `boxplot` function are (e.g., `las`), please use `?boxplot` to find out more. Information about `las` is located within `graphical parameters`.* 

Now, let's examine whether the `Treatment` and the `Type` variable have a significant effect on the `uptake` variable. This is a two way ANOVA!

```{r, two-way ANOVA,eval=FALSE}
CO2.aov <- aov(uptake~Treatment + Type, data = CO2)
summary(CO2.aov)
```

---

__Exercise 3__  
1) Please interpret the results of your two-way ANOVA. Which factors have a significant effect on CO2 uptake? 

---

# One-way and two-way ANOVAs on your own!
Now that you are a pro at running ANOVAs, let's use a different dataset and see if you can run one way and two way ANOVAs on this new dataset. Install the package `fivethirtyeight`. As a reminder, you can do this using:

```{r, load package,eval=FALSE}
install.packages('fivethirtyeight')
library(fivethirtyeight)
```

This is a great package that contains a ton of different datasets that were used in news stories on the blog 538. If you haven't read this blog yet, please check it out! It really bring home why what we're learning in this class is important for day to day life. To read more about the specific datasets in this package, you can check out this link [https://cran.r-project.org/web/packages/fivethirtyeight/fivethirtyeight.pdf].

Let's use the dataset called `bechdel`. This is a dataset on the amount of money different movies earned based on whether or not they passed the Bechdel test or not. For those of you who haven't heard about the bechdel test, you can read more about it here [https://en.wikipedia.org/wiki/Bechdel_test] but in a nutshell, for a movie to pass the Bechdel test it must:  
1) have at least two women who talk in the film  
2) they must talk to one another  
3) they need to talk about something other than a man  
Surprisingly many films still do not pass the Bechdel test! Think back to the last movie you saw.

---

__Exercise 4__  
1) Use an ANOVA to see if 2013 dollars (`domgross_2013`) differ based on whether a movie passed the Bechdel test or not (`binary`). Please write out the null and alternate hypothesis. Can you reject the null hypothesis at alpha = 0.05 based on the p value of your ANOVA? Please explain your results in non-technical terms.  
2) Now run an ANOVA to see if 2013 dollars (`domgross_2013`) differ based on whether a movie passed the Bechdel test or not (`binary`) and the decade in which the movie was made (`decade_code`). *Hint - Right now the `decade_code` variable is an integer and therefore not a categorical variable. To turn it into a categorical variable, please use 'as.factor(decade_code)' in your ANOVA function.*  
3) *BONUS/EXTRA CREDIT - worth 0.5% of total class grade* Pick another dataset from within the `fivethirtyeight` package and run either a one way or two way ANOVA. Please write out the null and alternate hypothesis and state whether you can reject the null hypothesis based on your p value. Remember, in order for a dataset to work for this question, your indepdent variable has to be categorical (and have 2 or more categories) and your dependent variable has to be continuous. You can ask Oscar/Arthur/Meha for help selecting an appropriate dataset, but please do not ask other students.

---